{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60540399-dfc1-4324-b61a-5b78094e806e",
   "metadata": {},
   "source": [
    "# X Ray Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3748ff71-a485-4e60-a9d5-7b08c51dd5bf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab17cb3f-df50-4595-9d1d-3cecb885979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7089076e-0df0-4da9-8bd0-fe4feff14dd8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Dataset Downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d82b9a1a-005c-4540-92f1-67975e5600a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "# import zipfile\n",
    "# from kaggle.api.kaggle_api_extended import KaggleApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f344f932-3c18-44a4-a9ea-48ca7ba9a6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api = KaggleApi()\n",
    "# api.authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f26c6264-29fb-4636-8f86-be3ce414b9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_identifier = \"tolgadincer/labeled-chest-xray-images\"\n",
    "# download_dir = \"../data/x-ray-images\"\n",
    "\n",
    "# os.makedirs(download_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9206991c-4334-47d4-b592-23f097cccf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download latest version\n",
    "# api.dataset_download_files(dataset_identifier, path=download_dir, unzip=True)\n",
    "\n",
    "# print(f\"Dataset downloaded to {download_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ec92b5-216b-4977-b815-43200f24ce20",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Dataset Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93abb68e-7382-4661-a6d7-83f3ec43c12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import models\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2a8a5ce-8b92-4d9b-bf9d-d4768cf02b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd4a644c-eb7f-4e3f-8a62-8bdd9d078af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"../data/chest_xray/train\"\n",
    "val_dir = \"../data/chest_xray/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ccca9dce-7547-46ad-ae68-35bdf1c341ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = v2.Compose([\n",
    "    v2.Grayscale(num_output_channels=3),\n",
    "    v2.Resize(size=(256, 256)), \n",
    "    # v2.RandomResizedCrop(size=(256, 256)),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.RandomRotation(degrees=15),\n",
    "    v2.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0)),\n",
    "    # v2.GaussianNoise(),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = v2.Compose([\n",
    "    v2.Grayscale(num_output_channels=3),\n",
    "    v2.Resize(size=(256, 256)),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1a122df5-63bd-4fd1-a944-bb0808738a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageFolder(root=train_dir, transform=train_transforms)\n",
    "val_dataset = ImageFolder(root=val_dir, transform=val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "906561c4-3b71-491c-a48d-a424bf711cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "06e62e43-8a78-436e-99b8-1dfdf572842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                          num_workers=4, pin_memory=False, prefetch_factor=1, \n",
    "                          persistent_workers=False, in_order=False)\n",
    "\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False, \n",
    "                        num_workers=4, pin_memory=False, prefetch_factor=1, \n",
    "                        persistent_workers=False, in_order=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4c69647-f240-4ec6-a1a5-7d12933b0c0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for idx, (x, y) in enumerate(val_loader):\n",
    "#     print(model.forward(x)[0][0])\n",
    "#     print(f\"Index: {idx} | Shape: {np.shape(x)} | Target length: {len(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273c07f8-092e-4da2-bf79-66a8b7b5677c",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08821240-d713-4c51-9bb7-c4176ec05cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class XRayNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.conv_relu_stack = nn.Sequential(\n",
    "#             nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=0, bias=True),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "#             nn.Conv2d(128, 256, kernel_size=5, stride=1, padding=1, bias=True),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=0, bias=True),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "#             nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=0, bias=True),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         )\n",
    "\n",
    "#         self.classifier = nn.Sequential(     \n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(430592, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(128, 1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv_relu_stack(x)\n",
    "#         x = self.classifier(x)\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "40e67508-d2bc-499b-a589-4f5433679f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_resnet18(num_classes=1):\n",
    "    weights = models.ResNet18_Weights\n",
    "    model = models.resnet18(weights=weights)\n",
    "\n",
    "    # Freeze initial layers\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(in_features, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, num_classes)\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bde582-66be-47a0-8a0e-3ccba1d4f08e",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a3aa1b22-ae63-44c8-83bc-b1e8d2a90731",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "model = initialize_resnet18(1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a4e22781-fedd-46aa-9bde-0b07d0817116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training script\n",
    "def train_model(model, train_loader, val_loader, batch_size=16, epochs=50, learning_rate=1e-3, log_dir='../reports/exp1'):\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "    \n",
    "    \n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}\\n------------------------------------------------------------------------------------\")\n",
    "        \n",
    "        # Training\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        total_train_loss = 0.0\n",
    "    \n",
    "        train_progress_bar = tqdm(train_loader, desc='Training', leave=True)\n",
    "        \n",
    "        for batch, (x, y) in enumerate(train_progress_bar):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            y_pred = model(x)\n",
    "            \n",
    "            train_loss = loss_fn(y_pred, y.unsqueeze(1).float())\n",
    "            train_loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += train_loss.item()\n",
    "            \n",
    "            train_progress_bar.set_postfix({'Batch Loss': f\"{train_loss.item():.3f}\"})\n",
    "\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "    \n",
    "        total_val_loss, correct = 0.0, 0\n",
    "        total = 0\n",
    "    \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_progress_bar = tqdm(val_loader, desc='Validation', leave=True)\n",
    "            \n",
    "            for x, y in val_progress_bar:\n",
    "                y_pred = model(x)\n",
    "                \n",
    "                val_loss = loss_fn(y_pred, y.unsqueeze(1).float())\n",
    "                total_val_loss += val_loss.item()\n",
    "                \n",
    "                val_progress_bar.set_postfix({'Val Loss': f\"{val_loss.item():.3f}\"})\n",
    "                \n",
    "                y_pred_labels = (torch.sigmoid(y_pred) > 0.5).int()\n",
    "                \n",
    "                correct += (y_pred_labels == y.unsqueeze(1).int()).sum().item()\n",
    "                total += y.size(0)\n",
    "                \n",
    "    \n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        accuracy = correct / total\n",
    "\n",
    "        writer.add_scalar('Loss/Validation', avg_val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', accuracy, epoch)\n",
    "\n",
    "        print(f\"Train Loss: {avg_train_loss:.3f} | Val Loss: {avg_val_loss:.3f} | Acccuracy: {(accuracy*100):.2f} % \\n\")\n",
    "        \n",
    "        writer.close()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d746e6fa-c140-4fe7-abe0-0c2c544a0e27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|████████████████████▍                                                                    | 75/327 [01:32<05:06,  1.21s/it, Batch Loss=0.290]"
     ]
    }
   ],
   "source": [
    "trained_model = train_model(model, train_loader, val_loader, epochs=1)\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd084def-0592-4e04-98b8-358fab3a9494",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trained_model, '../models/x_ray_classifier_resnet18-frozen.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c24544-c2c2-4ca6-8169-a28d554766ab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c121dca-5489-49db-b290-d6fc7ffdba0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep learning env",
   "language": "python",
   "name": "dl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
